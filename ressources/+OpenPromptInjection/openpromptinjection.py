import json
from typing import Literal

import OpenPromptInjection as PI
from OpenPromptInjection.utils import open_config

models = Literal["deepseek-llm-7b-chat", "deepseek-r1-distill-llama-8b", "deepseek-r1-distill-qwen-1.5b", "flan", "gpt", "internlm", "llama", "llama3", "mistral", "palm2", "vicuna"]
tasks = Literal["sst2", "sms_spam", "hsol", "mrpc", "jfleg", "rte", "gigaword"]
defenses = Literal["no", ]

# Parameters for the experiment
NumberOfRounds = 10
target_task: tasks = "sst2"
injected_task: tasks = "sms_spam"
model: models = "llama"
defense: defenses = "no"

# TODO Defense miteinbauen, scheints nichts eingebautes zu geben


# Create the target task
target_task = PI.create_task(open_config(config_path='./configs/task_configs/' + target_task + '_config.json'), NumberOfRounds)

# Create the model
model_config = open_config(config_path='./configs/model_configs/' + model + '_config.json')
model = PI.create_model(config=model_config)

# Create the injected task
inject_task = PI.create_task(open_config(config_path='./configs/task_configs/sms_spam_config.json'), NumberOfRounds, for_injection=True)
attacker = PI.create_attacker('combine', inject_task)

# Create the LLM-integrated App
target_app = PI.create_app(target_task, model, defense=defense)

# Queries the model with the attacked data prompt and save the responses
attack_responses = list()
for i, (data_prompt, ground_truth_label) in enumerate(target_app):
    data_prompt_after_attack = attacker.inject(data_prompt, i, target_task=target_task.task)
    response = target_app.query(data_prompt_after_attack, verbose=1, idx=i, total=len(target_app))
    attack_responses.append(response)

# Create an evaluator to calculate the ASV
evaluator = PI.create_evaluator(
    target_task_responses=None, 
    target_task=target_task,
    injected_task_responses=None, 
    injected_task=attacker.task,
    attack_responses=attack_responses
)

print(f"ASV = {evaluator.asv}")



# Collect all available metrics
metrics = {
    # "ASV": evaluator.asv,
    # Include other metrics if available in the evaluator
    "success_rate": evaluator.asv,
    "pna_i": evaluator.pna_i,
    "pna_t": evaluator.pna_t,
    "mr": evaluator.mr
}

# Write metrics to a JSON file
with open("evaluation_metrics.json", "w") as f:
    json.dump(metrics, f, indent=4)

print("Evaluation metrics written to 'evaluation_metrics.json'")