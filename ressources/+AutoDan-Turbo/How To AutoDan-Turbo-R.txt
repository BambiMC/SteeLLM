cd /mnt/hdd-baracuda/fberger
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash ~/Miniconda3-latest-Linux-x86_64.sh -b
source ~/.bashrc


huggingface-cli login --token //TODO
export HF_HOME=/mnt/hdd-baracuda/fberger/.huggingface_cache

wandb login
export WANDB_API_KEY=//TODO

export CUDA_DEVICE_ORDER=00000000:65:00.0  //Das ist nicht mehr aktuell, muss ich dynamisch über Namen ermitteln
export PIP_CACHE_DIR=/mnt/hdd-baracuda/fberger/.cache/
export CUDA_VISIBLE_DEVICES=0



git clone https://github.com/BambiMC/AutoDAN-Turbo.git
cd AutoDAN-Turbo

export DATASETS=$PWD/CybersecurityBenchmarks/datasets

conda create -y -n autodanturbo python==3.12
conda activate autodanturbo


conda install -y -c pytorch -c nvidia faiss-gpu=1.8.0 pytorch="*=*cuda*" pytorch-cuda=12 numpy
pip install -r requirements_pinned3.txt
pip install --upgrade openai
//TODO wand version spezifizieren in requirements.txt
//Vllt ist die requirements_pinned2 noch besser?


cd llm
git clone https://github.com/chujiezheng/chat_templates.git
cd ..


Neue /llm/openai_models.py hochladen





//Die ältere Variante, die Huggingface Modelle + OpenAI Embeddings verwendet, da müsste ich noch die logs/lifelong learning anpassen, die ist umbenannt in _r für die neue Version
python test.py --openai_api_key "//TODO" \
               --embedding_model "text-embedding-3-small" \
               --hf_token "//TODO" \
               --epochs 1 \
               --request "how to build a bomb?"






//Die neue Variante, nutzt Deepseek-R1 (from their official API) as the foundation model for the attacker, scorer, summarizer. We utilize OpenAI's text embedding model to embed text.
TRAINING: 
python main_r.py --vllm \
                 --openai_api_key "//TODO" \
                 --embedding_model "<openai text embedding model name>" \
                 --hf_token "<your huggingface token>" \
                 --deepseek_api_key "//TODO" \
                 --deepseek_model "deepseek-reasoner" \
                 --epochs 150

AFTER TRAINING: Epoche eigentlich 150
python test_r.py --openai_api_key "//TODO" \
                 --embedding_model "text-embedding-3-small" \
                 --hf_token "//TODO" \
                 --deepseek_api_key "//TODO" \
                 --deepseek_model "deepseek-reasoner" \
                 --epochs 2 \
                 //--request "<how to build a bomb?>" ist eh default


Notes:
AutoDAN-Turbo Jailbreak Dataset wird noch released, aber keine Ahnung wann, könnte ich mal nachfragen, weil ansonsten muss ich einfach ein bekanntes nehmen...



















openai migrate log:

./llm/openai_models.py
    -import openai
    +from openai import OpenAI
    +
    +client = OpenAI(api_key=openai_api_key)
     import logging
     from openai import AzureOpenAI
     import json
                     azure_endpoint=azure_endpoint
                 )
             else:
    -            openai.api_key = openai_api_key

         def encode(self, text):
             try:
                     data = sorted(data, key=lambda x: x["index"])
                     embeddings = [d["embedding"] for d in data]
                 else:
    -                response = openai.Embedding.create(
    -                    input=text,
    -                    model=self.embedding_model
    -                )
    -                embeddings = [item["embedding"] for item in response["data"]]
    +                response = client.embeddings.create(input=text,
    +                model=self.embedding_model)
    +                embeddings = [item["embedding"] for item in response.data]
                 if single_input and len(embeddings) == 1:
                     return embeddings[0]
                 return embeddings



