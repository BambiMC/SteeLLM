cd /mnt/hdd-baracuda/fberger


wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash ~/Miniconda3-latest-Linux-x86_64.sh -b	//MANUAL CLICKING INVOLVED???
source ~/.bashrc

git clone https://github.com/BambiMC/PurpleLlama.git

cd purplellama
conda create -n purplellama python==3.10
conda activate purplellama
pip install -r CybersecurityBenchmarks/requirements.txt
pip install torch
pip install transfomers
pip install huggingface_hub
pip install bitsandbytes accelerate


Neue Version der llm.py & local_llm hochladen 	//MANUAL

export PIP_CACHE_DIR=/mnt/hdd-baracuda/fberger/.cache/
export DATASETS=$PWD/CybersecurityBenchmarks/datasets
export HF_HOME="/mnt/hdd-baracuda/fberger/huggingface_cache/"
export CUDA_VISIBLE_DEVICES=0


//1i. MITRE and False Refusal Rate - (sollte funktionieren) - 16.3 GB VRAM
python3 -m CybersecurityBenchmarks.benchmark.run \
   --benchmark=mitre \
   --prompt-path="$DATASETS/mitre/mitre_benchmark_100_per_category_with_augmentation.json" \
   --response-path="$DATASETS/mitre_responses.json" \
   --judge-response-path="$DATASETS/mitre_judge_responses.json" \
   --stat-path="$DATASETS/mitre_stat.json" \
   --judge-llm="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO" \
   --expansion-llm="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO" \
   --llm-under-test="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO"

//MITRE and False Refusal Rate Multilingual - (sollte funktionieren) - 16.3 GB
python3 -m CybersecurityBenchmarks.benchmark.run \
   --benchmark=mitre \
   --prompt-path="$DATASETS/mitre/mitre_prompts_multilingual_machine_translated.json" \
   --response-path="$DATASETS/mitre_responses.json" \
   --judge-response-path="$DATASETS/mitre_judge_responses.json" \
   --stat-path="$DATASETS/mitre_stat.json" \
   --judge-llm="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO" \
   --expansion-llm="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO" \
   --llm-under-test="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO"

//1ii. False Refusal Rate -  - 7GB
python3 -m CybersecurityBenchmarks.benchmark.run \
   --benchmark=frr \
   --prompt-path="$DATASETS/frr/frr.json" \
   --response-path="$DATASETS/frr/frr_responses.json" \
   --stat-path="$DATASETS/frr/frr_stat.json" \
   --llm-under-test="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO"

//False Refusal Rate Multilingual -  - 7GB
python3 -m CybersecurityBenchmarks.benchmark.run \
   --benchmark=frr \
   --prompt-path="$DATASETS/frr/frr_multilingual_machine_translated.json" \
   --response-path="$DATASETS/frr/frr_responses.json" \
   --stat-path="$DATASETS/frr/frr_stat.json" \
   --llm-under-test="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO"

//2. Secure Code Generation 
Ist laut Doku gerade disabled, skippe ich for now

//3i. Normal PI - funktioniert - braucht 2 mal vram
python3 -m CybersecurityBenchmarks.benchmark.run \
   --benchmark=prompt-injection \
   --prompt-path="$DATASETS/prompt_injection/prompt_injection.json" \
   --response-path="$DATASETS/prompt_injection/prompt_injection_responses.json" \
   --judge-response-path="$DATASETS/prompt_injection/prompt_injection_judge_responses.json" \
   --stat-path="$DATASETS/prompt_injection/prompt_injection_stat.json" \
   --judge-llm="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO" \
   --llm-under-test="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO"

//PI mit Multilingual - (sollte funktionieren) - braucht 2 mal vram
python3 -m CybersecurityBenchmarks.benchmark.run \
   --benchmark=prompt-injection \
   --prompt-path="$DATASETS/prompt_injection/prompt_injection_multilingual_machine_translated.json" \
   --response-path="$DATASETS/prompt_injection/prompt_injection_responses.json" \
   --judge-response-path="$DATASETS/prompt_injection/prompt_injection_judge_responses.json" \
   --stat-path="$DATASETS/prompt_injection/prompt_injection_stat.json" \
   --judge-llm="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO" \
   --llm-under-test="LOCALLLM::meta-llama/Llama-2-7b-chat-hf:://TODO"

//3ii. Visual Prompt Injection
Skippe ich for now

// 4. Code Interpreter Tests
// evaluate the security risks posed by integrating LLMs with code interpreters
// 5. Vulnerability Exploitation Tests
// Asking the LLMs to solve "capture the flag" style challenges.
// 6. Spear Phishing Capability Tests
Geht um Cybersecurity Angriffe / Phising
// 7. Autonomous Offensive Cyber Operations Tests
Geht um Cybersecurity Angriffe
// 8. AutoPatch Tests
Geht um Security patches generieren f체r vulnerabilites (code?)




Notes: 
Help: python3 -m CybersecurityBenchmarks.benchmark.run --help
Ergebnisse in purplellama/CybersecurityBenchmarks/datasets/prompt_injection/
   <INPUT_DATASET> is one of prompt_injection.json (for English prompts) or prompt_injection_multilingual_machine_translated.json 
   --llm-under-test=<SPECIFICATION_2>	//F체r 2. Modell direkt mit rein
Im entg체ltigen nur die (l채ngeren) Multilinguals nutzen, damit breitere Abdeckung?
Ist Deutsch mit dabei? -> Ja



Konzentriert probiert:
microsoft/DialoGPT-small
meta-llama/Llama-2-7b-chat-hf

